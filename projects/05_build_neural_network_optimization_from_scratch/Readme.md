
About
=======
This project is inspired by Class 2 assignment in course "Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization" by Andrew Ng
on Coursera. In this project, different optimization methods, including Plain Gradient Descent (GD), mini-Batch GD, Momentum and Adam, are implemented from Scratch. 

Contents
----------

### 1. images/
This folder contains the required images used by source code. 

### 2. Source Codes

#### 2.1 optimization_for_neural_network.ipynb
Implementation of optimization methods in NN.
 
#### 2.2 opt_utils.py
Utility functions.

#### 2.3 testCases.py
Test cases.

### 3. datasets/
Train and test data. 

